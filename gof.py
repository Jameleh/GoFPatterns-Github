# -*- coding: utf-8 -*-
"""GoF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15C5e_Sq4BCqeWgssm38GrGDx_CHjOv3O
"""

import os

!pip install PyGithub

import csv
from github import Github

# List of all 23 GoF design patterns
gof_patterns = [
    "Abstract Factory", "Builder", "Prototype", "Singleton", "Factory Method",
    "Adapter", "Bridge", "Composite", "Decorator", "Facade", "Flyweight",
    "Proxy", "Chain of Responsibility", "Command", "Interpreter", "Iterator",
    "Mediator", "Memento", "Observer", "State", "Strategy", "Template Method",
    "Visitor"
]

# GitHub Personal Access Token
access_token = ""

# Initialize GitHub API client
g = Github(access_token)

# Function to search repositories and analyze them
def search_and_analyze_pattern(pattern):
    repositories_data = []
    # Searching repositories related to the GoF pattern
    search_query = f'"{pattern}" AND "design patterns"'
    repositories = g.search_repositories(query=search_query, sort="stars", order="desc")

    # Analyzing each repository
    for repo in repositories:
        repo_data = {
            "Pattern": pattern,
            "Repository Name": repo.name,
            "Description": repo.description,
            "Stars": repo.stargazers_count,
            "Forks": repo.forks_count,
            "created_year": repo.created_at.year,  # Extract year from timestamp,
            "updated_year": repo.updated_at.year,  # Extract last updated year
            "Contributors": len(list(repo.get_contributors())),
            "URL": repo.html_url
        }



        repositories_data.append(repo_data)

    return repositories_data

# Create or append data to a CSV file
def save_to_csv(data, filename="gof_patterns_analysis.csv"):
    # Write the headers only once, for the first time
    write_header = not os.path.exists(filename)

    with open(filename, mode='a', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=data[0].keys())
        if write_header:
            writer.writeheader()
        writer.writerows(data)




# Main analysis loop
def main():
    all_repositories_data = []

    # Iterate over each GoF pattern and collect the data
    for pattern in gof_patterns:
        print(f"Analyzing pattern: {pattern}")
        pattern_data = search_and_analyze_pattern(pattern)
        all_repositories_data.extend(pattern_data)

    # Save all collected data to CSV
    save_to_csv(all_repositories_data)
    print("Data saved to CSV.")

if __name__ == "__main__":
    main()